<p align="left">
    ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br><br>

<p align="center">
<a href='https://huggingface.co/spaces/zhichen'>
<img src='./images/logo.png'>
</a>
</p>

<div align="center">
  <p align="center">
    <h3> Llama3-Chinese </h3>

<p align="center">
      <a href='https://huggingface.co/zhichen'>
        <img src='https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Llama3%20Chinese-yellow'>
      </a>
      <a href='https://modelscope.cn/profile/seanzhang'>
        <img src='https://img.shields.io/badge/ğŸ¤– ModelScope-Llama3%20Chinese-blue'>
      </a>
      <br>
      <a href=href="https://github.com/seanzhang-zhichen/llama3-chinese/stargazers">
        <img src="https://img.shields.io/github/stars/seanzhang-zhichen/llama3-chinese?color=ccf">
      </a>
      <a href="https://github.com/seanzhang-zhichen/llama3-chinese/blob/main/LICENSE">
        <img alt="GitHub Contributors" src="https://img.shields.io/badge/license-Apache%202.0-blue.svg" />
      </a>
</p>
</div>


## ä»‹ç»

**Llama3-Chinese**æ˜¯**ä»¥Meta-Llama-3-8Bä¸ºåº•åº§**ï¼Œä½¿ç”¨ [DORA](https://arxiv.org/pdf/2402.09353.pdf) + [LORA+](https://arxiv.org/pdf/2402.12354.pdf) çš„è®­ç»ƒæ–¹æ³•ï¼Œåœ¨50wé«˜è´¨é‡ä¸­æ–‡å¤šè½®SFTæ•°æ® + 10wè‹±æ–‡å¤šè½®SFTæ•°æ® + 2000å•è½®è‡ªæˆ‘è®¤çŸ¥æ•°æ®è®­ç»ƒè€Œæ¥çš„å¤§æ¨¡å‹ã€‚

**Github:** [https://github.com/seanzhang-zhichen/llama3-chinese](https://github.com/seanzhang-zhichen/llama3-chinese)

![DEMO](./images/web_demo.png)


## æ¨¡å‹ä¸‹è½½

| Model             | Download  |
|:-------------------:|:-----------:|
| Meta-Llama-3-8B        |[ ğŸ¤— HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B) [  ğŸ¤– ModelScope](https://modelscope.cn/models/LLM-Research/Meta-Llama-3-8B)|
| Llama3-Chinese-Lora           |[ ğŸ¤— HuggingFace](https://huggingface.co/zhichen/Llama3-Chinese-Lora) [  ğŸ¤– ModelScope](https://modelscope.cn/models/seanzhang/Llama3-Chinese-Lora)|
| Llama3-Chinese (åˆå¹¶å¥½çš„æ¨¡å‹)           |[ ğŸ¤— HuggingFace](https://huggingface.co/zhichen/Llama3-Chinese) [  ğŸ¤– ModelScope](https://modelscope.cn/models/seanzhang/Llama3-Chinese)|



## åˆå¹¶LORAæ¨¡å‹ï¼ˆå¯è·³è¿‡ï¼‰

1ã€ä¸‹è½½ [Meta-Llama-3-8B](https://modelscope.cn/models/LLM-Research/Meta-Llama-3-8B)

```bash
git clone https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B.git
```

2ã€ä¸‹è½½[Llama3-Chinese-Lora](https://www.modelscope.cn/models/seanzhang/Llama3-Chinese-Lora)

**From ModelScope**
```bash
git lfs install
git clone https://www.modelscope.cn/seanzhang/Llama3-Chinese-Lora.git
```

**From HuggingFace**
```bash
git lfs install
git clone https://huggingface.co/zhichen/Llama3-Chinese-Lora
```

3ã€åˆå¹¶æ¨¡å‹

```bash
python merge_lora.py \
    --base_model path/to/Meta-Llama-3-8B \
    --lora_model path/to/lora/Llama3-Chinese-Lora  \
    --output_dir ./Llama3-Chinese
```

## ä¸‹è½½ Llama3-Chineseï¼ˆåˆå¹¶å¥½çš„æ¨¡å‹ï¼‰

**From ModelScope**
```bash
git lfs install
git clone https://www.modelscope.cn/seanzhang/Llama3-Chinese.git
```

**From HuggingFace**
```bash
git lfs install
git clone https://huggingface.co/zhichen/Llama3-Chinese
```


## æ¨ç†

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "zhichen/Llama3-Chinese"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype="auto", device_map="auto")

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "ä½ å¥½"},
]

input_ids = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True, return_tensors="pt"
).to(model.device)

outputs = model.generate(
    input_ids,
    max_new_tokens=2048,
    do_sample=True,
    temperature=0.7,
    top_p=0.95,
)
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))
```

## å‘½ä»¤è¡Œæ¨ç†

```bash
python cli_demo.py --model_path zhichen/Llama3-Chinese
```

## webæ¨ç†

```bash
python web_demo.py --model_path zhichen/Llama3-Chinese
```


## vllm web æ¨ç†

1ã€ä½¿ç”¨[vllm](https://github.com/vllm-project/vllm)éƒ¨ç½²æ¨¡å‹

```bash
python -m vllm.entrypoints.openai.api_server --served-model-name Llama3-Chinese --model ./Llama3-Chinese(æ¢æˆä½ è‡ªå·±çš„åˆå¹¶åçš„æ¨¡å‹è·¯å¾„)
```

2ã€åœ¨å‘½ä»¤è¡Œæ‰§è¡Œ

```bash
python vllm_web_demo.py --model Llama3-Chinese
```




## è®­ç»ƒæ•°æ®é›†

[åŒ æ•°ç§‘æŠ€å¤§æ¨¡å‹sftæ•°æ®é›†](https://modelscope.cn/datasets/deepctrl/deepctrl-sft-data)


## LICENSE

æœ¬é¡¹ç›®ä»…å¯åº”ç”¨äºç ”ç©¶ç›®çš„ï¼Œé¡¹ç›®å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®ï¼ˆåŒ…å«ä½†ä¸é™äºæ•°æ®ã€æ¨¡å‹ã€ä»£ç ç­‰ï¼‰å¯¼è‡´çš„å±å®³æˆ–æŸå¤±ã€‚è¯¦ç»†è¯·å‚è€ƒ[å…è´£å£°æ˜](https://github.com/seanzhang-zhichen/Llama3-Chinese/blob/main/DISCLAIMER)ã€‚

Llama3-Chineseé¡¹ç›®ä»£ç çš„æˆæƒåè®®ä¸º [The Apache License 2.0](./LICENSE)ï¼Œä»£ç å¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ï¼Œæ¨¡å‹æƒé‡å’Œæ•°æ®åªèƒ½ç”¨äºç ”ç©¶ç›®çš„ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ Llama3-Chineseçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚

## Citation

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†Llama3-Chineseï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š

```latex
@misc{Llama3-Chinese,
  title={Llama3-Chinese},
  author={Zhichen Zhang, Xin LU, Long Chen},
  year={2024},
  howpublished={\url{https://github.com/seanzhang-zhichen/llama3-chinese}},
}
```


## Acknowledgement

[meta-llama/llama3](https://github.com/meta-llama/llama3)
<br>
[hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)



## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=seanzhang-zhichen/Llama3-Chinese&type=Date)](https://star-history.com/#seanzhang-zhichen/Llama3-Chinese&Date)

